# HÃ£y fine-Tune model cá»§a báº¡n cho chá»©c nÄƒng function-calling

ChÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ fine-tune (tinh chá»‰nh) model Ä‘áº§u tiÃªn cho function-calling rá»“i Ä‘Ã¢y ğŸ”¥.

## LÃ m tháº¿ nÃ o Ä‘á»ƒ training model cho function-calling?

> CÃ¢u tráº£ lá»i: Ta cáº§n **data**

QuÃ¡ trÃ¬nh training model cÃ³ thá»ƒ chia thÃ nh 3 bÆ°á»›c:

1. **Model Ä‘Æ°á»£c pretrain trÃªn lÆ°á»£ng data khá»•ng lá»“**. Káº¿t quáº£ cá»§a bÆ°á»›c nÃ y lÃ  **pretrained model**. VÃ­ dá»¥: [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). ÄÃ¢y lÃ  model ná»n táº£ng vÃ  chá»‰ biáº¿t **dá»± Ä‘oÃ¡n token tiáº¿p theo mÃ  khÃ´ng cÃ³ kháº£ nÄƒng tuÃ¢n theo chá»‰ dáº«n**.

2. Äá»ƒ há»¯u Ã­ch trong bá»‘i cáº£nh chat, model cáº§n Ä‘Æ°á»£c **fine-tune** Ä‘á»ƒ tuÃ¢n theo hÆ°á»›ng dáº«n. á» bÆ°á»›c nÃ y, quÃ¡ trÃ¬nh training cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi nhÃ  phÃ¡t triá»ƒn model, cá»™ng Ä‘á»“ng mÃ£ nguá»“n má»Ÿ, báº¡n hay báº¥t ká»³ ai. VÃ­ dá»¥: [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) lÃ  model Ä‘Ã£ Ä‘Æ°á»£c fine-tune Ä‘á»ƒ tuÃ¢n theo chá»‰ dáº«n bá»Ÿi Ä‘á»™i ngÅ© Google cá»§a dá»± Ã¡n Gemma.

3. Model sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c **alignment** (cÃ¢n chá»‰nh) theo mong muá»‘n cá»§a ngÆ°á»i táº¡o. VÃ­ dá»¥: model chat há»— trá»£ khÃ¡ch hÃ ng khÃ´ng bao giá» Ä‘Æ°á»£c báº¥t lá»‹ch sá»±.

ThÃ´ng thÆ°á»ng cÃ¡c sáº£n pháº©m hoÃ n chá»‰nh nhÆ° Gemini hay Mistral **sáº½ tráº£i qua cáº£ 3 bÆ°á»›c**, trong khi cÃ¡c model báº¡n tÃ¬m tháº¥y trÃªn Hugging Face Ä‘Ã£ hoÃ n thÃ nh má»™t hoáº·c nhiá»u bÆ°á»›c training.

Trong hÆ°á»›ng dáº«n nÃ y, chÃºng ta sáº½ xÃ¢y dá»±ng model function-calling dá»±a trÃªn [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). Ta chá»n model Ä‘Ã£ fine-tune [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) thay vÃ¬ model ná»n táº£ng [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) vÃ¬ model Ä‘Ã£ fine-tune Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n cho use-case cá»§a ta.

Náº¿u báº¯t Ä‘áº§u tá»« pretrained model **sáº½ cáº§n training nhiá»u hÆ¡n Ä‘á»ƒ há»c cÃ¡ch tuÃ¢n theo chá»‰ dáº«n, chat VÃ€ function-calling**.

Báº±ng cÃ¡ch báº¯t Ä‘áº§u tá»« model Ä‘Ã£ fine-tune Ä‘á»ƒ tuÃ¢n theo chá»‰ dáº«n, **ta giáº£m thiá»ƒu lÆ°á»£ng thÃ´ng tin model cáº§n há»c**.

## LoRA (Low-Rank Adaptation of Large Language Models)

LoRA lÃ  ká»¹ thuáº­t training nháº¹ vÃ  phá»• biáº¿n giÃºp **giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ parameters cáº§n training**.

NÃ³ hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch **chÃ¨n má»™t lÆ°á»£ng nhá» weights má»›i vÃ o model nhÆ° adapter Ä‘á»ƒ training**. Äiá»u nÃ y giÃºp training vá»›i LoRA nhanh hÆ¡n, tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n, vÃ  táº¡o ra weights model nhá» hÆ¡n (vÃ i trÄƒm MB), dá»… lÆ°u trá»¯ vÃ  chia sáº».

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch thÃªm cÃ¡c cáº·p ma tráº­n phÃ¢n tÃ¡ch háº¡ng vÃ o cÃ¡c lá»›p Transformer, thÆ°á»ng táº­p trung vÃ o cÃ¡c lá»›p tuyáº¿n tÃ­nh. Trong quÃ¡ trÃ¬nh training, ta sáº½ "Ä‘Ã³ng bÄƒng" pháº§n cÃ²n láº¡i cá»§a model vÃ  chá»‰ cáº­p nháº­t weights cá»§a cÃ¡c adapter má»›i nÃ y.

Nhá» váº­y, sá»‘ **parameters** cáº§n training giáº£m Ä‘Ã¡ng ká»ƒ vÃ¬ ta chá»‰ cáº§n cáº­p nháº­t weights cá»§a adapter.

Trong quÃ¡ trÃ¬nh inference, Ä‘áº§u vÃ o sáº½ Ä‘i qua adapter vÃ  model ná»n táº£ng, hoáº·c cÃ¡c weights adapter cÃ³ thá»ƒ Ä‘Æ°á»£c há»£p nháº¥t vá»›i model ná»n táº£ng mÃ  khÃ´ng gÃ¢y thÃªm Ä‘á»™ trá»….

LoRA Ä‘áº·c biá»‡t há»¯u Ã­ch Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ **lá»›n** cho cÃ¡c tÃ¡c vá»¥ hoáº·c lÄ©nh vá»±c cá»¥ thá»ƒ trong khi váº«n quáº£n lÃ½ Ä‘Æ°á»£c yÃªu cáº§u tÃ i nguyÃªn. Äiá»u nÃ y giÃºp giáº£m bá»™ nhá»› **required** Ä‘á»ƒ training model.

Náº¿u muá»‘n tÃ¬m hiá»ƒu thÃªm vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a LoRA, hÃ£y xem [hÆ°á»›ng dáº«n nÃ y](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).

## Fine-Tuning (tinh chá»‰nh) Model cho Function-Calling

Báº¡n cÃ³ thá»ƒ truy cáº­p notebook hÆ°á»›ng dáº«n táº¡i Ä‘Ã¢y ğŸ‘‰ [Ä‘Ã¢y](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).

Sau Ä‘Ã³, click vÃ o [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) Ä‘á»ƒ cháº¡y notebook trÃªn Colab.