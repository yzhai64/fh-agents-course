# Using Tools in LlamaIndex

Within LlamaIndex, we build data agents with two core components:

1. A set of tools, discussed in [unit 1 on tools](../../unit1/tools.mdx)
2. A reasoning loop, discussed in [unit 1 on internal reasoning](../../unit1/thoughts.mdx)

## Using Tools in LlamaIndex

Defining a clear set of Tools is crucial to performance. As we discussed in [unit 1](../../unit1/tools.mdx), clear tool interfaces are easier for LLMs to use. Much like a software API interface for human engineers, they can get more out of the tool if it's easy to understand how it works.
LlamaIndex allows you to define a `Tool` as well as a `ToolSpec` containing a series of customise functions under the hood.

> When using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly on the tool name and description of the tools purpose and arguments.

Let's explore the main types of tools in LlamaIndex:

1. `FunctionTool`: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works.
2. `QueryEngineTool`: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools.
3. `Toolspecs`: Tools created by the community to work with different services like Gmail.
4. `Utility Tools`: Special tools that help handle large amounts of data from other tools.

We will go over each of these in more detail below.

### Creating a FunctionTool

A function tool is a simple wrapper around any existing python function.
We can choose to pass a sync or async function to the tool.
Additionally, we can choose to name and describe the tool as we want.
This is important because this information will be used by the agent to correctly use the tool.
The code below shows how to create a `FunctionTool` from a function.

```python
from llama_index.core.tools import FunctionTool

def get_weather(location: str) -> str:
    """Usfeful for getting the weather for a given location."""
    ...

tool = FunctionTool.from_defaults(
get_weather,
    # async_fn=aget_weather, name="...", description="...",
)
```

### Creating a QueryEngineTool

The `QueryEngine` we defined in the previous unit can be turned into a a tool using the `QueryEngineTool` class.
The code below shows how to create a `QueryEngineTool` from a `QueryEngine`.

```python
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.tools import QueryEngineTool
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPILM
from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding

embed_model = HuggingFaceInferenceAPIEmbedding("BAAI/bge-small-en-v1.5")
storage_context = StorageContext.from_defaults(persist_dir="path/to/vector/store")
index = load_index_from_storage(storage_context, embed_model=embed_model)

llm = HuggingFaceInferenceAPILM(model_name="meta-llama/Meta-Llama-3-8B-Instruct")
query_engine = index.as_query_engine(llm=llm)
tool = QueryEngineTool.from_defaults(
    query_engine,
    # name="...", description="..."
)
```

### Creating Toolspecs

Custom `Tools` and `ToolSpecs` are created by the community and shared on the [LlamaHub](https://llamahub.ai/).
You can think of `ToolSpecs` like a toolkit intended to be used together. Just like a professional toolkit, they cover relevant actions for a use case. For example, an accounting agent might have spreadsheet integration, an en email integration, and a calculator.

<details>
<summary>Google Toolspec</summary>
As introduced in the [section on components](what-are-components-in-llama-index.mdx), we can install the Google toolspec with the following command:

```python
pip install llama-index-tools-google
```
</details>

And now we can load the toolspec and convert it to a list of tools.

```python
from llama_index.tools.google import GmailToolSpec

tool_spec = GmailToolSpec()
tool_spec_list = tool_spec.to_tool_list()
```

### Utility Tools

Oftentimes, directly querying an API can return an excessive amount of data, some of which may be irrelevant, overflow the context window of the LLM, or unnecessarily increase the number of tokens that you are using.
Letâ€™s walk through our two main utility tools below.

1. `OnDemandToolLoader`: This tool turns any existing LlamaIndex data loader (BaseReader class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it 'on-demand'. All three of these steps happen in a single tool call.
2. `LoadAndSearchToolSpec`: The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a load tool and then a search tool. The load Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The search Tool execution would take in a query string as input and call the underlying index.

<Tip>You can find other utility tools on the [LlamaHub](https://llamahub.ai/)</Tip>

Now that we understand the basics of agents and tools in LlamaIndex, let's see how we can use LlamaIndex to create configurable and manageable workflows!