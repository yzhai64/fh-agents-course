# Messages and Special Tokens

Now that we understood how LLMs work, in this section we will explore how LLMs structure their generations through **tokenization**, **special tokens**, and **chat-templates**.

This is important to understand this section in order to be able to understand the Agent Re-act flow.

You have understood in the previous section that all LLMs have different EOS (End Of String) token. But this is not in fact no the only difference from one LLM to another. **Each LLM have it's own way to format prompt**. Yeah...

Also up until now we only talked about **Prompt** which are the tokens that goes as an input of your LLM. A prompt is universal for every LLM, they take a prompt as an input and complete the sequence with contextually probable words. 

> **Q**: But ... When, I'm interacting with chatGPT/Hugging Chat, I'm having a conversation in Messages, not prompts
>
> **A**: This is correct ! But this in fact mostly a UI thing. When fed into the LLM, the messages are concatenated back into a prompt.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Behind models"/>

## Messages: the underlying systems of LLMs

### System Messages

System messages (also called System Prompt) set the foundation for **how the model should behave**. They act as persistent instructions that influence all subsequent interactions. For example:

```python
system_message = {
    "role": "system",
    "content": "You are a professional customer service agent. Always be polite, clear, and helpful."
}
```

Based on our System Message, we can get a **polite Alfred**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Polite alfred"/>

Or a Rebel Alfred ðŸ˜Ž.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Rebel Alfred"/>

In addition to behavior, in Agents, the System Message allows **to store the information about available tools**, provide instructions to the model on how to format the actions to take, and guide the overall guidelines of how the thought process should be segmented.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Alfred System Prompt"/>

<!-- Todo: faire un lien avec conversations -->

### Conversations : User and Assistant Message

A conversation consist in alternating messages betwen a Human ( user ) and an LLM ( assistant )

Chat templates maintain context through conversation history, storing previous exchanges between users and the assistant. This allows for more coherent multi-turn conversations:

```python
conversation = [
    {"role": "user", "content": "I need help with my order"},
    {"role": "assistant", "content": "I'd be happy to help. Could you provide your order number?"},
    {"role": "user", "content": "It's ORDER-123"},
]
```

Templates can handle complex multi-turn conversations while maintaining context:

```python
messages = [
    {"role": "system", "content": "You are a math tutor."},
    {"role": "user", "content": "What is calculus?"},
    {"role": "assistant", "content": "Calculus is a branch of mathematics..."},
    {"role": "user", "content": "Can you give me an example?"},
]
```

<!-- Lien entre conversation et chat template -->

## Chat-Templates

Chat templates are essential for structuring interactions between language models and users. They provide instruction to format message conversations into prompt.

### Base Models vs Instruct Models

A base model is trained on raw text data to predict the next token, while an instruct model is fine-tuned specifically to follow instructions and engage in conversations. For example, `SmolLM2-135M` is a base model, while `SmolLM2-135M-Instruct` is its instruction-tuned variant.

To make a base model behave like an instruct model, we need to format our prompts in a consistent way that the model can understand. This is where chat templates come in. ChatML is one such template format that structures conversations with clear role indicators (system, user, assistant). If you have interacted with some AI API lately, you know that's the standard practice.

It's important to note that a base model could be fine-tuned on different chat templates, so when we're using an instruct model we need to make sure we're using the correct chat template. 

Here is an example :

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant focused on technical topics."},
    {"role": "user", "content": "Can you explain what a chat template is?"},
    {"role": "assistant", "content": "A chat template structures conversations between users and AI models..."}
]
```

### Understanding Chat Templates

Each model having different special token, chat templates have be implemented to ensure that we correctly format the prompt in each model. 

Chat templates include Jinja2 code on how to transform the ChatML list of JSON messages presented in the above example into a textual representation of the system-level instructions, user messages and assistant responses that the model can understand.

This structure helps maintain consistency across interactions and ensures the model responds appropriately to different types of inputs.Below is an example of a chat template:

chat_template of `SmolLM2-135M-Instruct`:
```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
You are a helpful AI assistant named SmolLM...
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```
As you can see a chat_template is some code that will write how should the list of messages be formated inside 

```sh
<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
```

If you remember last section, you will notice that "<|im_end|>" is the End of sequence ( EOS ) token of **SmolLM2-135M-Instruct**. Meaning that we only ask the assistant to generate some part of it ( in this case the assistant messages )

The `transformers` library will take care of chat templates for you in relation to the model's tokenizer. Read more about how transformers builds chat templates [here](https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-use-chat-templates). All we have to do is structure our messages in the correct way and the tokenizer will take care of the rest.

Or you can experiment with different conversations/models to see how they are then formated for the model in the following space :

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>


<!-- Move later -->
### Format messages to prompt

So while you are interracting with your AI through messages. To ensure the correct format of your conversation, the easiest is to get the chat_template from the model's tokenizers and to format your prompt with apply_chat_template() function

```python
messages = [
    {"role": "system", "content": "You are an AI assistant with access to various tools."},
    {"role": "user", "content": "Hi !"},
    {"role": "assistant", "content": "Hi human, what can help you with ?"},
]
```

```python
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
```

The rendered_prompt  out of this function is no ready to go in the specific model you chose !
> This apply_chat_template() is used in the backend of your API, if you are interacting with the messages ( ChatML ) format.

<!-- Todo add transition -->

## Resources

- [Hugging Face Chat Templating Guide](https://huggingface.co/docs/transformers/main/en/chat_templating)
- [Transformers Documentation](https://huggingface.co/docs/transformers)

